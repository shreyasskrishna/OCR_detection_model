# 🚀 OCR Model Conversion: GPU to CPU 🖥️🔄

Welcome to the mini project where I explored the **conversion of an OCR (Optical Character Recognition) model from GPU to CPU**. This project dives into performance comparisons between different hardware setups to evaluate the **efficiency of OCR tasks**.

---

## 🔍 Project Overview

This project is aimed at understanding how OCR model performance varies between **GPU and CPU executions**. It involves:

- 🔄 Converting a GPU-accelerated OCR model to run on CPU
- 🧠 Analyzing **frame-per-second (FPS)** metrics
- 📊 Comparing **processed frames per second**
- 🔧 Gaining insights into **hardware optimization** for real-world ML applications

---

## 📌 Key Highlights

- ✅ **Investigated OCR model performance** on both GPU and CPU
- ⚙️ **Compared FPS** and frame processing speeds across hardware
- 📈 Documented findings to highlight real-time efficiency differences
- 🧪 Enhanced practical knowledge in **hardware-aware model deployment**

---

## 💡 Why This Matters

Understanding the performance gap between CPU and GPU execution for OCR models is essential for:

- 🚀 Optimizing real-time applications
- 💻 Deploying models on devices with limited hardware (e.g., edge devices)
- 📦 Making informed decisions during **model deployment and scaling**

---

## 🛠️ Tech Stack

- 🐍 Python
- 📦 OpenCV / Tesseract / PyTorch (depending on model used)
- 🧠 OCR Model (custom/pretrained)
- 🖥️ System performance tracking tools (FPS calculator, timer utilities)

---

## 📁 Repository Structure

